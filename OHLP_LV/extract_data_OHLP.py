import re
import json
import os

from tqdm import tqdm
from concurrent.futures import ProcessPoolExecutor, as_completed

import sys
sys.path.append("")

from utils.extract_text_from_pdf import extract_text_from_pdf

def split_medications(s):
    # –ò—â–µ–º —à–∞–±–ª–æ–Ω: —á—Ç–æ-—Ç–æ + [...] –≤ –∫–æ–Ω—Ü–µ —Å—Ç—Ä–æ–∫–∏
    # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –∏—â–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ "+[" –ø–µ—Ä–µ–¥ –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–π —Å–∫–æ–±–∫–æ–π –≤ –∫–æ–Ω—Ü–µ —Å—Ç—Ä–æ–∫–∏
    match = re.search(r'^(.*?)\+(\[.*?\])$', s.strip())
    
    if match:
        main_part = match.group(1)
        bracket_part = match.group(2)
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —á–∞—Å—Ç—å –ø–æ "+"
        main_list = [part.strip() for part in main_part.split('+')] if main_part else []
        
        # –£–±–∏—Ä–∞–µ–º —Å–∫–æ–±–∫–∏ —É –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —á–∞—Å—Ç–∏ –∏ —Ä–∞–∑–¥–µ–ª—è–µ–º –ø–æ "+"
        inner_bracket = bracket_part[1:-1]  # —É–±–∏—Ä–∞–µ–º [ –∏ ]
        bracket_list = [part.strip() for part in inner_bracket.split('+')] if inner_bracket else []
        
        return main_list, bracket_list
    else:
        # –ï—Å–ª–∏ —à–∞–±–ª–æ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî –ø—Ä–æ—Å—Ç–æ –¥–µ–ª–∏–º –≤—Å—é —Å—Ç—Ä–æ–∫—É –ø–æ "+"
        return [part.strip() for part in s.split('+')], []


def extract_filename_info(filename):
    # 1. –†–∞–∑–¥–µ–ª–∏—Ç—å –ø–æ –ø–µ—Ä–≤–æ–π ")" ‚Äî –ø–æ–ª—É—á–∏–º –Ω–æ–º–µ—Ä –∏ –æ—Å—Ç–∞—Ç–æ–∫
    num_prepare, rest = filename.split(')', 1)

    # 2. –£–±—Ä–∞—Ç—å –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –æ—Å—Ç–∞—Ç–∫–∞
    rest = rest.strip()

    # 3. –†–∞–∑–¥–µ–ª–∏—Ç—å –æ—Å—Ç–∞—Ç–æ–∫ –ø–æ " _ " ‚Äî –æ–∂–∏–¥–∞–µ–º 3 —á–∞—Å—Ç–∏: –ú–ù–ù, —Ç–æ—Ä–≥–æ–≤–æ–µ, –¥–∞—Ç–∞.pdf
    parts = rest.split(' _ ')
    if len(parts) != 3:
        raise ValueError("–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å—Ç—Ä–æ–∫–∏")

    prepare_name_string = parts[0].strip()  # –Ω–∞–ø—Ä–∏–º–µ—Ä, "–ì–ª–∏–∫–ª–∞–∑–∏–¥+–ú–µ—Ç—Ñ–æ—Ä–º–∏–Ω"
    trade_name = parts[1].strip()           # –Ω–∞–ø—Ä–∏–º–µ—Ä, "–ì–ª–∏–∫–ª–∞–¥–∞¬Æ"
    date_pdf = parts[2].strip()             # –Ω–∞–ø—Ä–∏–º–µ—Ä, "02.11.2022.pdf"

    # 4. –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –ú–ù–ù: –µ—Å–ª–∏ –µ—Å—Ç—å "+", —Ä–∞–∑–±–∏—Ç—å –Ω–∞ —Å–ø–∏—Å–æ–∫
    if prepare_name_string != "~":
        international_name, international_name_dlc = split_medications(prepare_name_string)
    else:
        international_name, international_name_dlc = "~", "~"

    # 5. –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∞—Ç—É: –æ—Ç—Ä–µ–∑–∞—Ç—å ".pdf"
    if date_pdf.endswith('.pdf'):
        date = date_pdf[:-4]

    return {
        'num_prepare': int(num_prepare),
        'international_name': international_name,
        'international_name_dlc': international_name_dlc,
        'trade_name': trade_name,
        'date': date
    }

def split_text_by_chapters(text):
    """
    –†–∞–∑–¥–µ–ª—è–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –≥–ª–∞–≤—ã –ø–æ —Å–º—ã—Å–ª–æ–≤—ã–º –∑–∞–≥–æ–ª–æ–≤–∫–∞–º.
    –£—á–∏—Ç—ã–≤–∞–µ—Ç:
      - –ù–æ–º–µ—Ä–∞ –≥–ª–∞–≤ –≤ —Ç–µ–∫—Å—Ç–µ –º–æ–≥—É—Ç –±—ã—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, "4.8 –ù–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∞–∫—Ü–∏–∏") –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å ("–ù–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∞–∫—Ü–∏–∏")
      - –†–µ–≥–∏—Å—Ç—Ä, –ø—Ä–æ–±–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, –æ–ø–µ—á–∞—Ç–∫–∏
      - –ü–æ—Ä—è–¥–æ–∫ –≥–ª–∞–≤ —Å—Ç—Ä–æ–≥–∏–π ‚Äî —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏–∑–≤–ª–µ–∫–∞–µ—Ç—Å—è –º–µ–∂–¥—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º–∏ –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      - —Å–ª–æ–≤–∞—Ä—å {—ç—Ç–∞–ª–æ–Ω–Ω—ã–π_–∑–∞–≥–æ–ª–æ–≤–æ–∫_–±–µ–∑_–Ω–æ–º–µ—Ä–∞: content}
      - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É: —Å–∫–æ–ª—å–∫–æ –Ω–∞–π–¥–µ–Ω–æ, –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∫–∞–∫–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã
    """

    # –≠—Ç–∞–ø 1: –û–ø—Ä–µ–¥–µ–ª—è–µ–º —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ –∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç—ã (–≤ –Ω—É–∂–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ!)
    variants_map = {
        '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ª–µ–∫–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–ø–∞—Ä–∞—Ç–∞': [
            '–ù–ê–ò–ú–ï–ù–û–í–ù–ò–ï –õ–ï–ö–ê–†–°–¢–í–ï–ù–ù–û–ì–û –ü–†–ï–ü–ê–†–ê–¢–ê'
        ],
        '–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Å–æ—Å—Ç–∞–≤': [
            '–ö–ê–ß–ï–°–¢–í–ï–ù–ù–¨I–ô –ò –ö–û–õ–ò–ß–ï–°–¢–í–ï–ù–ù–´–ô –°–û–°–¢–ê–í',
            '–ö–ê–ß–ï–°–¢–í–ï–ù–ù–´–ô –ò –ö–û–õ–ò–ß–ï–°–¢–í–ï–ù–ù–¨I–ô –°–û–°–¢–ê–í',
            '–ö–ê–ß–ï–°–¢–í–ï–ù–ù–¨I–ô –ò –ö–û–õ–ò–ß–ï–°–¢–í–ï–ù–ù–¨I–ô –°–û–°–¢–ê–í',
            '–ö–ê–ß–ï–°–¢–í–ï–ù–ù–´–ô –ò –ö–û–õ–ò–ß–ï–°–¢–í–ï–ù–ù–´–ô CO–°–¢–ê–í',
            '–ö –ê–ß–ï–°–¢–í–ï–ù–ù–´–ô –ò –ö–û–õ–ò–ß–ï–°–¢–í–ï–ù–ù–´–ô –°–û–°–¢–ê–í',
        ],
        '–õ–µ–∫–∞—Ä—Å—Ç–≤–µ–Ω–Ω–∞—è —Ñ–æ—Ä–º–∞': [],
        '–ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ': [
            '–ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–π –¥–∞–Ω–Ω—ã–µ'
        ],
        '–ü–æ–∫–∞–∑–∞–Ω–∏—è –∫ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é': [],
        '–†–µ–∂–∏–º –¥–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å–ø–æ—Å–æ–± –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è': [],
        '–ü—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è': [],
        '–û—Å–æ–±—ã–µ —É–∫–∞–∑–∞–Ω–∏—è –∏ –º–µ—Ä—ã –ø—Ä–µ–¥–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏': [],
        '–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ –ª–µ–∫–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø—Ä–µ–ø–∞—Ä–∞—Ç–∞–º–∏ –∏ –¥—Ä—É–≥–∏–µ –≤–∏–¥—ã –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è': [
            '–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ –ª–µ–∫–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø—Ä–µ–ø–∞—Ä–∞—Ç–∞–º–∏',
            '–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –¥—Ä—É–≥–∏–º–∏ –ª–µ–∫–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø—Ä–µ–ø–∞—Ä–∞—Ç–∞–º–∏',
            '–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ –ª–µ–∫–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Å—Ä–µ–¥—Å—Ç–≤–∞–º–∏',
            '–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ –ª–µ–∫–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Å—Ä–µ–¥—Å—Ç–≤–∞–º–∏ –∏ –¥—Ä—É–≥–∏–µ –≤–∏–¥—ã',
            '–í –∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ –ª–µ–∫–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø—Ä–µ–ø–∞—Ä–∞—Ç–∞–º–∏ –∏ –¥—Ä—É–≥–∏–µ'
        ],
        '–§–µ—Ä—Ç–∏–ª—å–Ω–æ—Å—Ç—å, –±–µ—Ä–µ–º–µ–Ω–Ω–æ—Å—Ç—å –∏ –ª–∞–∫—Ç–∞—Ü–∏—è': [
            '–§–µ—Ä—Ç–∏–ª—å–Ω–æ—Å—Ç—å, –±–µ—Ä–µ–º–µ–Ω–Ω–æ—Å—Ç—å –∏ –ª–∞–∫—Ç–∞—Ü–∏–∏',
            '–§–µ—Ä—Ç–∏–ª—å–Ω–æ—Å—Ç—å, –±–µ—Ä–µ–º–µ–Ω–Ω–æ—Å—Ç—å –∏ –ª–∞–∫—Ç–∞—Ü–∏—è',
            '–ë–µ—Ä–µ–º–µ–Ω–Ω–æ—Å—Ç—å –∏ –ª–∞–∫—Ç–∞—Ü–∏—è',
            '–§–µ—Ä—Ç–∏–ª—å–Ω–æ—Å—Ç—å, –±–µ—Ä–µ–º–µ–Ω–Ω–æ—Å—Ç—å –∏ –ø–µ—Ä–∏–æ–¥ –≥—Ä—É–¥–Ω–æ–≥–æ –≤—Å–∫–∞—Ä–º–ª–∏–≤–∞–Ω–∏—è',
            '–ë–µ—Ä–µ–º–µ–Ω–Ω–æ—Å—Ç—å, –≥—Ä—É–¥–Ω–æ–µ –≤—Å–∫–∞—Ä–º–ª–∏–≤–∞–Ω–∏–µ –∏ —Ñ–µ—Ä—Ç–∏–ª—å–Ω–æ—Å—Ç—å',
            '–§–µ—Ä—Ç–∏–ª—å–Ω–æ—Å—Ç—å, –±–µ—Ä–µ–º–µ–Ω–Ω–æ—Å—Ç—å –∏ –∫–æ—Ä–º–ª–µ–Ω–∏–µ –≥—Ä—É–¥—å—é'
        ],
        '–í–ª–∏—è–Ω–∏–µ –Ω–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —É–ø—Ä–∞–≤–ª—è—Ç—å —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—ã–º–∏ —Å—Ä–µ–¥—Å—Ç–≤–∞–º–∏ –∏ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –º–µ—Ö–∞–Ω–∏–∑–º–∞–º–∏': [
            '–í–ª–∏—è–Ω–∏–µ –Ω–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —É–ø—Ä–∞–≤–ª—è—Ç—å',
            '–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—ã–º–∏ —Å—Ä–µ–¥—Å—Ç–≤–∞–º–∏ –∏ —Ä–∞–±–æ—Ç–∞ —Å –º–µ—Ö–∞–Ω–∏–∑–º–∞–º–∏',
            '–í–ª–∏—è–Ω–∏–µ –Ω–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —É–ø—Ä–∞–≤–ª—è—Ç—å —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—ã–º–∏ —Å—Ä–µ–¥—Å—Ç–≤–∞–º–∏ –∏ —Ä–∞–±–æ—Ç–∞—Ç—å —Å'
        ],
        '–ù–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∞–∫—Ü–∏–∏': [
            '–ü–æ–±–æ—á–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ',
            '–ü–æ–±–æ—á–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è'
        ],
        '–ü–µ—Ä–µ–¥–æ–∑–∏—Ä–æ–≤–∫–∞': [],
        '–§–∞—Ä–º–∞–∫–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞': [],
        '–§–∞—Ä–º–∞–∫–æ–¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞': [
            '–§–∞—Ä–º–∞–∫–æ–¥–∏–Ω–∞–º–∏–∫–∞'
        ],
        '–§–∞—Ä–º–∞–∫–æ–∫–∏–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞': [
            '–§–∞—Ä–º–∞–∫–æ–∫–∏–Ω–µ—Ç–∏–∫–∞'
        ],
        '–î–∞–Ω–Ω—ã–µ –¥–æ–∫–ª–∏–Ω–∏—á–µ—Å–∫–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏': [
            '–î–æ–∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –ø–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏'
        ],
        '–§–∞—Ä–º–∞—Ü–µ–≤—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞': [
            '–§–ê–†–ú–ê–¶–ï–í–¢–ò–ß–ïC–ö–ò–ï –°–í–û–ô–°–¢–í–ê',
            '–§–ê–†–ú–ê–¶–ï–í–¢–ò–ß–ï–ö–ò–ï –°–í–û–ô–°–¢–í–ê'
        ],
    }

    trash_headers = {
        '–ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ',
        '–§–∞—Ä–º–∞—Ü–µ–≤—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞',
        '–î–∞–Ω–Ω—ã–µ –¥–æ–∫–ª–∏–Ω–∏—á–µ—Å–∫–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏',
        '–§–∞—Ä–º–∞–∫–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞',
        '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ª–µ–∫–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–ø–∞—Ä–∞—Ç–∞'
    }

    # –≠—Ç–∞–ø 2: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    def make_pattern(header_text):
        tokens = re.split(r'\s+', header_text.strip())
        escaped_tokens = [re.escape(token) for token in tokens]
        flexible_header = r'\s+'.join(escaped_tokens)
        # –ò—â–µ–º: (–Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –Ω–æ–º–µ—Ä –≥–ª–∞–≤—ã) + –∑–∞–≥–æ–ª–æ–≤–æ–∫, –±–µ–∑ –ø—Ä–∏–≤—è–∑–∫–∏ –∫ –Ω–∞—á–∞–ª—É —Å—Ç—Ä–æ–∫–∏
        pattern = r'(?i)(?<!\w)(?:\s*(?:\d+(?:\.\d+)*)\s*)?' + flexible_header + r'(?!\w)'
        # pattern = r'(?i)(?<=\n)(?:\s*(?:\d+(?:\.\d+)*)\s+)?' + flexible_header + r'(?=\n)'
        # pattern = r'(?i)(?:^|\n)\s*(?:\d+(?:\.\d+)*)?\s*' + flexible_header + r'(?=\s|$)'
        return pattern
    
    def clean_chapter_tail(content):
        """
        –£–¥–∞–ª—è–µ—Ç —Å –∫–æ–Ω—Ü–∞ —Ç–µ–∫—Å—Ç–∞ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –≤–∏–¥–∞:
            "4.", "\n5.", " \n\n6.1 ", –∏ —Ç.–ø.
        –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è, –∫–∞–∫ —Ç–æ–ª—å–∫–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç "–Ω–æ—Ä–º–∞–ª—å–Ω—ã–π" —Ç–µ–∫—Å—Ç.
        """
        # –£–¥–∞–ª—è–µ–º —Å –∫–æ–Ω—Ü–∞: –ø—Ä–æ–±–µ–ª—ã, –ø–µ—Ä–µ–Ω–æ—Å—ã, —Ü–∏—Ñ—Ä—ã, —Ç–æ—á–∫–∏, –Ω–æ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ –í–°–Å ‚Äî —Ö–≤–æ—Å—Ç
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–≥—É–ª—è—Ä–∫—É: —É–±–∏—Ä–∞–µ–º —Å –∫–æ–Ω—Ü–∞ –ª—é–±—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑ \s, \d, .
        # –ù–æ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∞ –ù–ï –ø—Ä–µ—Ä—ã–≤–∞–µ—Ç—Å—è –±—É–∫–≤–∞–º–∏/—Å–∏–º–≤–æ–ª–∞–º–∏
        # (?<=\S) ‚Äî —á—Ç–æ–±—ã –Ω–µ –æ–±—Ä–µ–∑–∞—Ç—å, –µ—Å–ª–∏ –ø–µ—Ä–µ–¥ —ç—Ç–∏–º –±—ã–ª –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
        # –ù–æ –ø—Ä–æ—â–µ: –ø—Ä–æ—Å—Ç–æ —É–¥–∞–ª—è–µ–º —Å –∫–æ–Ω—Ü–∞ –¥–æ –ø–µ—Ä–≤–æ–≥–æ "–Ω–µ-–∞—Ä—Ç–µ—Ñ–∞–∫—Ç–Ω–æ–≥–æ" —Å–∏–º–≤–æ–ª–∞

        # –†–µ–≥—É–ª—è—Ä–∫–∞: —É–¥–∞–ª—è–µ–º —Å –∫–æ–Ω—Ü–∞ —Å—Ç—Ä–æ–∫–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å: –ø—Ä–æ–±–µ–ª—ã, —Ü–∏—Ñ—Ä—ã, —Ç–æ—á–∫–∏, –ø–µ—Ä–µ–Ω–æ—Å—ã
        # –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –æ–Ω–∏ –∏–¥—É—Ç –ø–æ–¥—Ä—è–¥ –∏ –Ω–µ –ø—Ä–µ—Ä—ã–≤–∞—é—Ç—Å—è "–Ω–æ—Ä–º–∞–ª—å–Ω—ã–º" —Ç–µ–∫—Å—Ç–æ–º
        content = re.sub(r'[\s\d.]*$', '', content)
        return content.strip()

    # –≠—Ç–∞–ø 3: –°–æ–±–∏—Ä–∞–µ–º –í–°–ï –≤—Ö–æ–∂–¥–µ–Ω–∏—è –í–°–ï–• –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ
    all_matches = []  # [(start, end, canonical, matched_text), ...]

    for canonical, variants in variants_map.items():
        candidates = [canonical] + variants
        for cand in candidates:
            pattern = make_pattern(cand)
            for match in re.finditer(pattern, text, re.MULTILINE | re.IGNORECASE):
                all_matches.append((match.start(), match.end(), canonical, match.group(0)))

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ–∑–∏—Ü–∏–∏ –≤ —Ç–µ–∫—Å—Ç–µ
    all_matches.sort(key=lambda x: x[0])

    # –≠—Ç–∞–ø 4: –§–∏–ª—å—Ç—Ä—É–µ–º, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ –ü–ï–†–í–û–ï –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞,
    # –ø—Ä–∏—á—ë–º –≤ –ø–æ—Ä—è–¥–∫–µ, –∑–∞–¥–∞–Ω–Ω–æ–º variants_map (—á—Ç–æ–±—ã –Ω–µ –Ω–∞—Ä—É—à–∞—Ç—å –ø–æ—Ä—è–¥–æ–∫ –≥–ª–∞–≤!)
    seen_canonicals = set()
    ordered_headers = []  # –±—É–¥–µ–º –¥–æ–±–∞–≤–ª—è—Ç—å —Ç–æ–ª—å–∫–æ –≤ –ø–æ—Ä—è–¥–∫–µ variants_map

    for start, end, canonical, matched_text in all_matches:
        if canonical in seen_canonicals:
            continue
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ —Å–ª–µ–¥—É—é—â–∏–π –æ–∂–∏–¥–∞–µ–º—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –ø–æ –ø–æ—Ä—è–¥–∫—É
        # (—ç—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –º—ã –Ω–µ "–ø–µ—Ä–µ–ø—Ä—ã–≥–Ω–µ–º" —á–µ—Ä–µ–∑ –≥–ª–∞–≤—É, –∫–æ—Ç–æ—Ä–∞—è –¥–æ–ª–∂–Ω–∞ –∏–¥—Ç–∏ —Ä–∞–Ω—å—à–µ)
        if canonical not in seen_canonicals:
            seen_canonicals.add(canonical)
            # –ù–æ –¥–æ–±–∞–≤–ª—è–µ–º –≤ ordered_headers —Ç–æ–ª—å–∫–æ –≤ –ø–æ—Ä—è–¥–∫–µ —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞
            # ‚Äî –ø–æ—ç—Ç–æ–º—É –ø—Ä–æ—Å—Ç–æ –∑–∞–ø–æ–º–Ω–∏–º –≤—Å–µ –ø–µ—Ä–≤—ã–µ –≤—Ö–æ–∂–¥–µ–Ω–∏—è, –∞ –ø–æ—Ç–æ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ø–æ—Ä—è–¥–∫—É
            # (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: –º–æ–∂–Ω–æ –±—ã–ª–æ –±—ã –ø–µ—Ä–µ–±–∏—Ä–∞—Ç—å variants_map –∏ –∏—Å–∫–∞—Ç—å –ø–µ—Ä–≤–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ)

    # –¢–µ–ø–µ—Ä—å –ø–µ—Ä–µ—Å—Ç—Ä–∞–∏–≤–∞–µ–º ordered_headers –≤ —Å—Ç—Ä–æ–≥–æ–º –ø–æ—Ä—è–¥–∫–µ variants_map
    # –ë–µ—Ä—ë–º –ø–µ—Ä–≤–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞, –µ—Å–ª–∏ –æ–Ω–æ –µ—Å—Ç—å, –≤ –ø–æ—Ä—è–¥–∫–µ —Å–ø–∏—Å–∫–∞
    final_headers = []
    for canonical in variants_map.keys():
        for match in all_matches:
            if match[2] == canonical:
                final_headers.append(match)
                break  # —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ

    # –≠—Ç–∞–ø 5: –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –º–µ–∂–¥—É –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
    found_chapters = {}

    for i in range(len(final_headers)):
        start_pos = final_headers[i][1]  # –∫–æ–Ω–µ—Ü —Ç–µ–∫—É—â–µ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        canonical = final_headers[i][2]

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º "–º—É—Å–æ—Ä–Ω—ã–µ" –≥–ª–∞–≤—ã
        if canonical in trash_headers:
            continue

        # –ö–æ–Ω–µ—Ü –±–ª–æ–∫–∞ ‚Äî –Ω–∞—á–∞–ª–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏–ª–∏ –∫–æ–Ω–µ—Ü —Ç–µ–∫—Å—Ç–∞
        end_pos = final_headers[i + 1][0] if i + 1 < len(final_headers) else len(text)
        content = text[start_pos:end_pos]

        # –û—á–∏—Å—Ç–∫–∞ —Ö–≤–æ—Å—Ç–∞
        content = clean_chapter_tail(content)

        found_chapters[canonical] = content

    # –≠—Ç–∞–ø 6: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    found_canonicals = set(found_chapters.keys())
    expected_headers = set(variants_map.keys()) - trash_headers
    not_found_list = sorted(expected_headers - found_canonicals, key=list(variants_map.keys()).index)

    # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ–∞–∫—Ü–∏–π
    # (—É–¥–∞–ª–µ–Ω–∏–µ "–°–æ–æ–±—â–µ–Ω–∏–µ –æ –ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º—ã—Ö –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ–∞–∫—Ü–∏—è—Ö")
    side_e_text = found_chapters.get("–ù–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∞–∫—Ü–∏–∏", "")
    cut_marker = "\n–°–æ–æ–±—â–µ–Ω–∏–µ –æ –ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º—ã—Ö –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ–∞–∫—Ü–∏—è—Ö\n"
    if cut_marker in text:
        found_chapters["–ù–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∞–∫—Ü–∏–∏"] = side_e_text.split(cut_marker)[0]

    result = {
        'chapters': found_chapters,
        'stats': {
            'found_count': len(found_chapters),
            'not_found_count': len(not_found_list),
            'not_found_list': not_found_list
        }
    }

    return result

def process_OHLP_pdf(pdf_path):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç PDF-—Ñ–∞–π–ª –û–õ–°/–ü/–û–í–õ–° –∏
    –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≥–ª–∞–≤–∞–º."""
    
    _, filename = os.path.split(pdf_path)
    pdf_text = extract_text_from_pdf(pdf_path, margin_bottom=70, join_dash=True)
    
    filename_info = extract_filename_info(filename)
    chapters_stats = split_text_by_chapters(pdf_text)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–ª–æ–≤–∞—Ä–∏: –µ—Å–ª–∏ –µ—Å—Ç—å –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–π ‚Äî chapters_stats –∏–º–µ–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
    return {**filename_info, **chapters_stats}
    
    # return {
    #     filename_info | chapters_stats
    #     # # –ò–∑ extract_filename_info()
    #     # 'num_prepare': filename_info.get('num_prepare', None),
    #     # 'international_name': filename_info.get('international_name', None),
    #     # 'international_name_dlc': filename_info.get('international_name_dlc', None),
    #     # 'trade_name': filename_info.get('trade_name', None),
    #     # 'date': filename_info.get('date', None),

    #     # # split_text_by_chapters()
    #     # 'chapters': chapters_stats.get('chapters', None),
    #     # 'stats': chapters_stats.get('stats', None) 
    # }

def report_missing_chapters(data, save_path):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á—ë—Ç –ø–æ –Ω–µ–Ω–∞–π–¥–µ–Ω–Ω—ã–º –≥–ª–∞–≤–∞–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –µ–≥–æ –≤ —Ñ–∞–π–ª.
    :param data: —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–ø–∞—Ä–∞—Ç–æ–≤ (dict)
    :param save_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á—ë—Ç–∞ (—Å—Ç—Ä–æ–∫–∞)
    """
    lines = []
    lines.append("üîç –ù–µ–Ω–∞–π–¥–µ–Ω–Ω—ã–µ –≥–ª–∞–≤—ã –ø–æ –ø—Ä–µ–ø–∞—Ä–∞—Ç–∞–º:\n")

    any_missing = False

    for drug in data:
        num_prepare = drug["num_prepare"]
        international_name = " + ".join(drug["international_name"])
        trade_name = drug["trade_name"]
        date = drug["date"]

        not_found_list = drug["stats"]["not_found_list"]

        if not_found_list and len(not_found_list) < 13:
            any_missing = True
            lines.append("‚Äî" * 80)
            lines.append(f"üíä –ü—Ä–µ–ø–∞—Ä–∞—Ç ‚Ññ{num_prepare}: {international_name} ({trade_name}) ‚Äî {date}")
            lines.append("   üßæ –ù–µ–Ω–∞–π–¥–µ–Ω–Ω—ã–µ –≥–ª–∞–≤—ã:")
            for header in not_found_list:
                lines.append(f"      ‚ùå {header}")

    if not any_missing:
        lines.append("‚úÖ –í—Å–µ –≥–ª–∞–≤—ã –Ω–∞–π–¥–µ–Ω—ã –¥–ª—è –≤—Å–µ—Ö –ø—Ä–µ–ø–∞—Ä–∞—Ç–æ–≤.")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
    with open(save_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

    print(f"‚úÖ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {save_path}")


def filter_entries(data):
    """
    –§–∏–ª—å—Ç—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π –ø–æ –∑–∞–¥–∞–Ω–Ω—ã–º —É—Å–ª–æ–≤–∏—è–º.

    –£—Å–ª–æ–≤–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–∞–ø–∏—Å–∏:
    - stats['found_count'] == 13
    - –ò–õ–ò stats['found_count'] == 12 –∏ stats['not_found_list'] == ["–§–∞—Ä–º–∞–∫–æ–∫–∏–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞"]

    –í—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∑–∞–ø–∏—Å–∏ –ø–æ–ø–∞–¥–∞—é—Ç –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è (–≤–æ–∑–≤—Ä–∞—â–∞—é—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ).

    Args:
        data (list): –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∫–ª—é—á–∞–º–∏ 'stats', 'international_name', 'trade_name'

    Returns:
        dict: –°–ª–æ–≤–∞—Ä—å —Å –¥–≤—É–º—è –∫–ª—é—á–∞–º–∏:
            - 'kept': —Å–ø–∏—Å–æ–∫ –∑–∞–ø–∏—Å–µ–π, —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è—é—â–∏—Ö —É—Å–ª–æ–≤–∏—è–º
            - 'removed': —Å–ø–∏—Å–æ–∫ –∑–∞–ø–∏—Å–µ–π, –Ω–µ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è—é—â–∏—Ö —É—Å–ª–æ–≤–∏—è–º
    """
    kept = []
    removed = []

    for item in data:
        stats = item.get("stats", {})
        found_count = stats.get("found_count")
        not_found_list = stats.get("not_found_list", [])

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ª–æ–≤–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        if found_count == 13 or (
            found_count == 12 and not_found_list == ["–§–∞—Ä–º–∞–∫–æ–∫–∏–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞"]
        ):
            kept.append(item)
        else:
            removed.append({
                "num_prepare": item.get("num_prepare", None),
                "international_name": item.get("international_name", None),
                "trade_name": item.get("trade_name", None),
                "not_found_list": not_found_list
            })

    return {
        "kept": kept,
        "removed": removed
    }



if __name__ == "__main__":
    OHLP_RESULT_FILENAME = "OHLP_LV\\data\\OHLP_all.json"
    OHLP_DIR = "OHLP_LV\\data\\–û–•–õ–ü_all"

    pdf_files = [os.path.join(OHLP_DIR, f) for f in os.listdir(OHLP_DIR) if f.endswith('.pdf')]

    OHKP_parsed = []

    # –†–∞—Å–ø–∞—Ä–∞–ª–ª–µ–ª–∏–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
    with ProcessPoolExecutor() as executor:
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á–∏
        future_to_path = {executor.submit(process_OHLP_pdf, path): path for path in pdf_files}

        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
        for future in tqdm(as_completed(future_to_path), total=len(pdf_files), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤", unit="—Ñ–∞–π–ª"):
            try:
                result = future.result()
                OHKP_parsed.append(result)
            except Exception as exc:
                path = future_to_path[future]
                print(f"\n–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {path}: {exc}")

    OHKP_parsed.sort(key=lambda x: x['num_prepare'])

    OHKP_result = filter_entries(OHKP_parsed)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON
    with open(OHLP_RESULT_FILENAME, "w", encoding="utf-8") as json_file:
        json.dump(OHKP_result, json_file, ensure_ascii=False, indent=4)

    print(f"\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {OHLP_RESULT_FILENAME}")

    report_missing_chapters(OHKP_parsed, 'OHLP_LV\\data\\missing_chapters.txt')