import pandas as pd
import numpy as np
import re
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm
import random
import json

class SemanticConnectionChecker:
    def __init__(self, model_path, dataset_path, abbr_dataset = None, sep=';', 
                 normalization_threshold=0.92):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π —Ç–µ—Ä–º–∏–Ω–æ–≤.
        
        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            model_path: –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
            dataset_path: –ø—É—Ç—å –∫ CSV (source;target)
            sep: —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
            normalization_threshold: –ø–æ—Ä–æ–≥ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤ (—á–µ–º –≤—ã—à–µ ‚Äî —Å—Ç—Ä–æ–∂–µ)
        """
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
        self.model = SentenceTransformer(model_path)
        self.normalization_threshold = normalization_threshold
        
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å–≤—è–∑–µ–π...")
        self.df = pd.read_csv(dataset_path, sep=sep, header=0)

        # –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–ª–æ–≤–∞—Ä—è: –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª—é—á–∞ –∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞ —Å–æ–∑–¥–∞—ë–º –ø–∞—Ä—É variant ‚Üí key
        if abbr_dataset:
            self.abbrev_map = {
                variant.strip(): key.strip()
                for key, variants in abbr_dataset.items()
                for variant in variants  # variants –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ —Å–ø–∏—Å–æ–∫
            }
            print("–ó–∞–≥—Ä—É–∂–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä")
        else:
            print("–î–∞—Ç–∞—Å–µ—Ç –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω")
            self.abbrev_map = None
        
        # –û—á–∏—Å—Ç–∫–∞ –æ—Ç —Å–∫–æ–±–æ–∫
        self.df['source_clean'] = self.df['source'].apply(self._clean_term)
        self.df['target_clean'] = self.df['target'].apply(self._clean_term)
        
        # –í—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –æ—á–∏—â–µ–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        all_cleaned = pd.concat([self.df['source_clean'], self.df['target_clean']]).dropna().unique()
        self.unique_terms = [t for t in all_cleaned if t.strip()]
        
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(self.unique_terms)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        self.term_embeddings = self.model.encode(
            self.unique_terms,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
        # –ú–∞—Ç—Ä–∏—Ü–∞ —Å—Ö–æ–¥—Å—Ç–≤ (N x N)
        print("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã —Å—Ö–æ–¥—Å—Ç–≤ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏...")
        sim_matrix = cosine_similarity(self.term_embeddings)
        
        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è: –Ω–∞—Ö–æ–¥–∏–º –≥—Ä—É–ø–ø—ã –ø–æ—Ö–æ–∂–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        print("–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ—Ö–æ–∂–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤...")
        clusters = self._cluster_similar_terms(sim_matrix)
        
        # –°–æ–∑–¥–∞—ë–º –º–∞–ø–ø–∏–Ω–≥: —Å—Ç–∞—Ä—ã–π_—Ç–µ—Ä–º–∏–Ω ‚Üí –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π_—Ç–µ—Ä–º–∏–Ω
        self.normalization_map = {}
        for cluster in clusters:
            if len(cluster) > 1:
                # –í—ã–±–∏—Ä–∞–µ–º —Å–∞–º—ã–π –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ—Ä–º–∏–Ω –∫–∞–∫ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π (—ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
                # canonical = min(cluster, key=lambda x: len(x))
                canonical = random.choice(cluster)  # —Å–ª—É—á–∞–π–Ω—ã–π –≤—ã–±–æ—Ä –∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–∞
                for term in cluster:
                    if term != canonical:
                        self.normalization_map[term] = canonical
        
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(self.normalization_map)} –∑–∞–º–µ–Ω –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.")
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –∫ –¥–∞—Ç–∞—Å–µ—Ç—É
        self.df['source_normalized'] = self.df['source_clean'].map(
            lambda x: self.normalization_map.get(x, x)
        )
        self.df['target_normalized'] = self.df['target_clean'].map(
            lambda x: self.normalization_map.get(x, x)
        )

        self.embeddings_matrix = np.array(self.term_embeddings)
        
        print("–ì–æ—Ç–æ–≤–æ! –ü—Ä–æ–≤–µ—Ä—è–ª—å—â–∏–∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏.")

    def _expand_abbreviations(self, phrase: str) -> str:
        """
        –ó–∞–º–µ–Ω—è–µ—Ç –≤—Å–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã, —Å–∏–º–≤–æ–ª—ã –∏–ª–∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –∏–∑ self.abbrev_map
        –Ω–∞ –∏—Ö –ø–æ–ª–Ω—ã–µ —Ñ–æ—Ä–º—ã (–∫–ª—é—á–∏).
        –£—á–∏—Ç—ã–≤–∞–µ—Ç –≥—Ä–∞–Ω–∏—Ü—ã —Å–ª–æ–≤: –∑–∞–º–µ–Ω—è–µ—Ç 'Œ≤', 'alpha' ‚Üí '–±–µ—Ç–∞', '–∞–ª—å—Ñ–∞'
        —Ç–æ–ª—å–∫–æ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –µ–¥–∏–Ω–∏—Ü—ã.
        –†–∞–±–æ—Ç–∞–µ—Ç —Å —Ä–µ–≥–∏—Å—Ç—Ä–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ.

        :param phrase: –≤—Ö–æ–¥–Ω–∞—è —Ñ—Ä–∞–∑–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–±–ª–æ–∫–∞—Ç–æ—Ä Œ≤-—Ä–µ—Ü–µ–ø—Ç–æ—Ä–æ–≤")
        :return: —Ñ—Ä–∞–∑–∞ —Å –∑–∞–º–µ–Ω—ë–Ω–Ω—ã–º–∏ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–±–ª–æ–∫–∞—Ç–æ—Ä –±–µ—Ç–∞-—Ä–µ—Ü–µ–ø—Ç–æ—Ä–æ–≤")
        """

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø–æ –¥–ª–∏–Ω–µ (—Å–Ω–∞—á–∞–ª–∞ –¥–ª–∏–Ω–Ω—ã–µ ‚Äî —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —á–∞—Å—Ç–∏—á–Ω—ã—Ö –∑–∞–º–µ–Ω, –Ω–∞–ø—Ä–∏–º–µ—Ä, "al" –Ω–µ –∑–∞–º–µ–Ω–∏–ª "alpha")
        sorted_variants = sorted(self.abbrev_map.keys(), key=len, reverse=True)
        result = phrase

        for variant in sorted_variants:
            full_form = self.abbrev_map[variant]
            escaped = re.escape(variant)
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º negative lookbehind –∏ negative lookahead –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≥—Ä–∞–Ω–∏—Ü
            pattern = rf'(?<![a-zA-Z–∞-—è–ê-–Ø]){escaped}(?![a-zA-Z–∞-—è–ê-–Ø])'
            result = re.sub(pattern, full_form, result, flags=re.IGNORECASE)

        return phrase
    
    def _clean_term(self, term):
        """–£–¥–∞–ª—è–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤ —Å–∫–æ–±–∫–∞—Ö, –≤–∫–ª—é—á–∞—è —Å–∫–æ–±–∫–∏, —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤—ã–≤–∞–µ—Ç –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã"""
        term = term.lower()
        term = re.sub(r'\([^)]*\)', '', str(term)).strip()
        if self.abbrev_map:
            term = self._expand_abbreviations(term)
        return term
    
    def _cluster_similar_terms(self, sim_matrix):
        """
        –ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç —Ç–µ—Ä–º–∏–Ω—ã –≤ –∫–ª–∞—Å—Ç–µ—Ä—ã –ø–æ –ø–æ—Ä–æ–≥—É —Å—Ö–æ–¥—Å—Ç–≤–∞.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–≥–ª–æ–º–µ—Ä–∞—Ç–∏–≤–Ω—É—é –ª–æ–≥–∏–∫—É —á–µ—Ä–µ–∑ –≥—Ä–∞—Ñ —Å–≤—è–∑–µ–π.
        """
        n = len(self.unique_terms)
        visited = np.zeros(n, dtype=bool)
        clusters = []
        
        for i in range(n):
            if visited[i]:
                continue
            # –ù–æ–≤—ã–π –∫–ª–∞—Å—Ç–µ—Ä
            cluster = [self.unique_terms[i]]
            visited[i] = True
            
            # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö, –∫—Ç–æ —Å–≤—è–∑–∞–Ω —Å i –∏–ª–∏ —Å –∫–µ–º-—Ç–æ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
            stack = [i]
            while stack:
                idx = stack.pop()
                for j in range(n):
                    if not visited[j] and sim_matrix[idx, j] >= self.normalization_threshold:
                        cluster.append(self.unique_terms[j])
                        visited[j] = True
                        stack.append(j)
            
            if len(cluster) > 1:
                clusters.append(cluster)
        
        return clusters
    

    
    def check_pair(self, term1, term2, similarity_threshold=0.92):
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–≤—è–∑—å –º–µ–∂–¥—É –¥–≤—É–º—è —Ç–µ—Ä–º–∏–Ω–∞–º–∏ —á–µ—Ä–µ–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç.
        """

        cleaned_term1 = self._clean_term(term1)
        cleaned_term2 = self._clean_term(term2)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤—Ö–æ–¥–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã (–µ—Å–ª–∏ –µ—Å—Ç—å –≤ –º–∞–ø–ø–∏–Ω–≥–µ)
        norm_term1 = self.normalization_map.get(cleaned_term1, cleaned_term1)
        norm_term2 = self.normalization_map.get(cleaned_term2, cleaned_term2)
        
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        input_embs = self.model.encode([cleaned_term1, cleaned_term2], convert_to_numpy=True)
        emb1 = input_embs[0].reshape(1, -1)
        emb2 = input_embs[1].reshape(1, -1)
        
        # –ü–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–∏—Ö –≤ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–∞—Ö
        sims_to_1 = cosine_similarity(emb1, self.term_embeddings)[0]
        sims_to_2 = cosine_similarity(emb2, self.term_embeddings)[0]
        
        similar_to_1 = [(self.unique_terms[i], sims_to_1[i]) 
                       for i in range(len(sims_to_1)) if sims_to_1[i] >= similarity_threshold]
        similar_to_2 = [(self.unique_terms[i], sims_to_2[i]) 
                       for i in range(len(sims_to_2)) if sims_to_2[i] >= similarity_threshold]
        
        set1 = {self.normalization_map.get(t, t) for t, s in similar_to_1}
        set2 = {self.normalization_map.get(t, t) for t, s in similar_to_2}
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–≤—è–∑—å –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
        for _, row in self.df.iterrows():
            src_norm = row['source_normalized']
            tgt_norm = row['target_normalized']
            if src_norm in set1 and tgt_norm in set2:
                sim1 = max([s for t, s in similar_to_1 
                           if self.normalization_map.get(t, t) == src_norm], default=0)
                sim2 = max([s for t, s in similar_to_2 
                           if self.normalization_map.get(t, t) == tgt_norm], default=0)
                return {
                    'has_connection': True,
                    'similar_source': src_norm,
                    'similar_target': tgt_norm,
                    'similarity_term1': sim1,
                    'similarity_term2': sim2,
                    'original_mapping_source': [k for k, v in self.normalization_map.items() if v == src_norm],
                    'original_mapping_target': [k for k, v in self.normalization_map.items() if v == tgt_norm],
                    'input_term1': term1,
                    'input_term2': term2
                }
            if src_norm in set2 and tgt_norm in set1:
                sim2 = max([s for t, s in similar_to_1 
                           if self.normalization_map.get(t, t) == tgt_norm], default=0)
                sim1 = max([s for t, s in similar_to_2 
                           if self.normalization_map.get(t, t) == src_norm], default=0)
                return {
                    'has_connection': True,
                    'similar_source': tgt_norm,
                    'similar_target': src_norm,
                    'similarity_term1': sim1,
                    'similarity_term2': sim2,
                    'original_mapping_source': [k for k, v in self.normalization_map.items() if v == tgt_norm],
                    'original_mapping_target': [k for k, v in self.normalization_map.items() if v == src_norm],
                    'input_term1': term1,
                    'input_term2': term2
                }
        
        # üî¥ –ï—Å–ª–∏ —Å–≤—è–∑—å –ù–ï –Ω–∞–π–¥–µ–Ω–∞ ‚Äî –≤—Å—ë —Ä–∞–≤–Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã
        return {
            'has_connection': False,
            'input_term1': term1,
            'input_term2': term2,
            'normalized_inputs': (norm_term1, norm_term2),
            'synonyms_term1': [t for t, s in similar_to_1],
            'similarity_scores_term1': [(t, s) for t, s in similar_to_1],
            'synonyms_term2': [t for t, s in similar_to_2],
            'similarity_scores_term2': [(t, s) for t, s in similar_to_2],
            'threshold': similarity_threshold
        }
    

if __name__ == "__main__":

    SYNONYM_FILENAME = "data\\dictonary_synonims_simple.json"
    with open(SYNONYM_FILENAME, 'r', encoding='utf-8') as file:
        abbr_dataset = json.load(file)['abbrev']

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è (–æ–¥–∏–Ω —Ä–∞–∑)
    checker = SemanticConnectionChecker(
        model_path='train_synonim_model\\data\\synonym-model_1',
        dataset_path='process_yEd_graph\\data\\list_edges_verified_folder.csv',
        abbr_dataset=abbr_dataset,
        sep=';',
        normalization_threshold=0.965
    )

    print("\n" + "üîç –°–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Å–≤—è–∑–µ–π –∑–∞–ø—É—â–µ–Ω–∞")
    print("–í–≤–µ–¥–∏—Ç–µ 'exit' –≤ –ª—é–±–æ–º –ø–æ–ª–µ, —á—Ç–æ–±—ã –≤—ã–π—Ç–∏\n")

    while True:
        print("-" * 60)
        term_1 = input("–ü–µ—Ä–≤—ã–π —Ç–µ—Ä–º–∏–Ω: ").strip()
        if term_1.lower() == 'exit':
            break
        term_2 = input("–í—Ç–æ—Ä–æ–π —Ç–µ—Ä–º–∏–Ω: ").strip()
        if term_2.lower() == 'exit':
            break

        # === –®–∞–≥ 1: –ü–æ–∫–∞–∑—ã–≤–∞–µ–º, –∫–∞–∫ —Ç–µ—Ä–º–∏–Ω—ã –±—ã–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã ===
        cleaned1 = checker._clean_term(term_1)
        cleaned2 = checker._clean_term(term_2)

        norm1 = checker.normalization_map.get(cleaned1, "–Ω–µ –∑–∞–º–µ–Ω—ë–Ω")
        norm2 = checker.normalization_map.get(cleaned2, "–Ω–µ –∑–∞–º–µ–Ω—ë–Ω")

        print("\nüìù –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–≤–æ–¥–∞:")
        print(f"  '{term_1}' ‚Üí –æ—á–∏—â–µ–Ω–æ: '{cleaned1}'")
        if norm1 != "–Ω–µ –∑–∞–º–µ–Ω—ë–Ω":
            print(f"\t\t\t‚Üí –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ: '{norm1}' (—Å–∏–Ω–æ–Ω–∏–º)")
        else:
            print(f"\t\t\t‚Üí –±–µ–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏")

        print(f"  '{term_2}' ‚Üí –æ—á–∏—â–µ–Ω–æ: '{cleaned2}'")
        if norm2 != "–Ω–µ –∑–∞–º–µ–Ω—ë–Ω":
            print(f"\t\t\t‚Üí –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ: '{norm2}' (—Å–∏–Ω–æ–Ω–∏–º)")
        else:
            print(f"\t\t\t‚Üí –±–µ–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏")

        # === –®–∞–≥ 2: –ü–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞) ===
        emb1 = checker.model.encode([cleaned1], convert_to_numpy=True).reshape(1, -1)
        emb2 = checker.model.encode([cleaned2], convert_to_numpy=True).reshape(1, -1)

        sims1 = cosine_similarity(emb1, checker.embeddings_matrix)[0]
        sims2 = cosine_similarity(emb2, checker.embeddings_matrix)[0]

        top5_1 = sorted(zip(checker.unique_terms, sims1), key=lambda x: x[1], reverse=True)[:5]
        top5_2 = sorted(zip(checker.unique_terms, sims2), key=lambda x: x[1], reverse=True)[:5]

        print("\nüîç –ë–ª–∏–∂–∞–π—à–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ:")
        print("  –ü–æ—Ö–æ–∂–∏–µ –Ω–∞ –ø–µ—Ä–≤—ã–π:")
        for t, s in top5_1:
            mark = " ‚Üê –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω" if checker.normalization_map.get(t) == norm1 or t == norm1 else ""
            print(f"\t‚Ä¢ {t} (—Å—Ö–æ–∂–µ—Å—Ç—å: {s:.3f}){mark}")

        print("\t–ü–æ—Ö–æ–∂–∏–µ –Ω–∞ –≤—Ç–æ—Ä–æ–π:")
        for t, s in top5_2:
            mark = " ‚Üê –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω" if checker.normalization_map.get(t) == norm2 or t == norm2 else ""
            print(f"\t‚Ä¢ {t} (—Å—Ö–æ–∂–µ—Å—Ç—å: {s:.3f}){mark}")

        # === –®–∞–≥ 3: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–≤—è–∑–∏ ===
        result = checker.check_pair(
            term_1,
            term_2,
            similarity_threshold=0.962
        )

        print("\n" + ("‚úÖ –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ê–Ø –°–í–Ø–ó–¨ –ù–ê–ô–î–ï–ù–ê!" if result['has_connection'] else "‚ùå –°–≤—è–∑—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"))
        print("‚Äî " * 30)

        if result['has_connection']:
            print(f"üîó –ù–∞–π–¥–µ–Ω–∞ —Å–≤—è–∑—å —á–µ—Ä–µ–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã:")
            print(f"   {result['similar_source']}  ‚îÄ‚îÄ‚îÄ‚Üí  {result['similar_target']}")

            print(f"\nüìä –°—Ç–µ–ø–µ–Ω—å —Å—Ö–æ–∂–µ—Å—Ç–∏:")
            print(f"\t'{term_1}' ‚Üí '{result['similar_source']}': {result['similarity_term1']:.3f}")
            print(f"\t'{term_2}' ‚Üí '{result['similar_target']}': {result['similarity_term2']:.3f}")

            if result['original_mapping_source']:
                print(f"\nüîÑ –°–∏–Ω–æ–Ω–∏–º—ã –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {', '.join(result['original_mapping_source'])}")
            if result['original_mapping_target']:
                print(f"üîÑ –°–∏–Ω–æ–Ω–∏–º—ã —Ü–µ–ª–∏: {', '.join(result['original_mapping_target'])}")

            # print(f"\nüìÑ –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è —Å–≤—è–∑—å –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ:")
            # print(f"   {result['original_source']} ‚Üí {result['original_target']}")
        else:
            print(f"‚ÑπÔ∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –ø—Ä—è–º—É—é —Å–≤—è–∑—å –º–µ–∂–¥—É:")
            print(f"\t'{result['input_term1']}' –∏ '{result['input_term2']}'.")

            print(f"\nüîç –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã (–ø–æ—Ä–æ–≥: {result['threshold']:.3f}):")
            if result['synonyms_term1']:
                print(f"  –ü–æ—Ö–æ–∂–∏–µ –Ω–∞ '{term_1}':")
                for t, s in result['similarity_scores_term1']:
                    mark = " ‚Üê –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω" if t == result['normalized_inputs'][0] else ""
                    print(f"\t‚Ä¢ {t} (—Å—Ö–æ–∂–µ—Å—Ç—å: {s:.3f}){mark}")
            else:
                print(f"  üö´ –ù–µ—Ç —Ç–µ—Ä–º–∏–Ω–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ, –ø–æ—Ö–æ–∂–∏—Ö –Ω–∞ '{term_1}'")

            if result['synonyms_term2']:
                print(f"  –ü–æ—Ö–æ–∂–∏–µ –Ω–∞ '{term_2}':")
                for t, s in result['similarity_scores_term2']:
                    mark = " ‚Üê –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω" if t == result['normalized_inputs'][1] else ""
                    print(f"\t‚Ä¢ {t} (—Å—Ö–æ–∂–µ—Å—Ç—å: {s:.3f}){mark}")
            else:
                print(f"  üö´ –ù–µ—Ç —Ç–µ—Ä–º–∏–Ω–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ, –ø–æ—Ö–æ–∂–∏—Ö –Ω–∞ '{term_2}'")

            print(f"\nüí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–¥–∏–Ω –∏–∑ —ç—Ç–∏—Ö —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ, –µ—Å—Ç—å –ª–∏ —Å–º—ã—Å–ª–æ–≤–∞—è —Ü–µ–ø–æ—á–∫–∞.")

        print("\n")